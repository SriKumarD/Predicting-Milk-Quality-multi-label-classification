{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56d716a1-b144-4f17-aae2-830f9d7adee1",
   "metadata": {},
   "source": [
    "# Predicting Milk Quality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0652c275-2b70-4926-87cf-4e1bf7ba8da8",
   "metadata": {},
   "source": [
    "# Introduction and Overview\n",
    "\n",
    "For this assignment, we are using the milk dataset taken from : https://www.kaggle.com/datasets/cpluzshrijayan/milkquality?resource=download and got approved by got approved by professor Timothy Smith.\n",
    "\n",
    "### A description of the data The information is gathered from a state's various milk producers. Here we have taken the 5 categories of data from the samples collected. Those are the ph (acidity level of the milk), The texture of the milk , the taste of the milk , the fat percentage of the milk , the turbility (calarity) of the milk , and the grade of the milk .\n",
    "\n",
    "### Units of the data collected from the producers :\n",
    "\n",
    "##### 1. The logarithemic units represent the pH of the milk.\n",
    "\n",
    "##### 2. The milk's temperature is measured in Fahrenheit.\n",
    "\n",
    "##### 3. The taste is represented as a binary integer, where 0 indicates a bad taste and 1 indicates a good taste.\n",
    "\n",
    "##### 4. The odour is taken as a binary unit where 0 means a bad odour from the milk and 1 means a good odour from the milk \n",
    "\n",
    "##### 5. The fat percentage of milk is taken as 0 means good 1 for bad\n",
    "\n",
    "##### 6. Turbidity refers to the milk's clarity.We were using the binary units that 0 means no turbidity of the milk and 1 means good turbidity of the milk.\n",
    "\n",
    "##### 7. There are 9 catagories of colours of milk \n",
    "\n",
    "##### 8. The grade is classified into three levels: low, medium, and high.\n",
    "\n",
    "\n",
    "\n",
    "### Aim : Here we are trying to find which milk is the best for consumption by calculating the accuracy of model. The grade is the target variable  and the other columns are predictors. Our main goal in this assignment is to find which milk is best for consumption with the help of predictors using different data models .In the preceding data, grade is used to calculate the multi-level classification. It is the target variable, and it has three classifications: low, medium, and high."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd499af-aa59-4d59-94be-eedcc779d092",
   "metadata": {},
   "source": [
    "\n",
    "# Importing all Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8a4d3cef-e972-4bca-a0e1-7dfc43d34d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as pltB\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score ,f1_score,classification_report, make_scorer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import export_text\n",
    "from sklearn.utils import resample\n",
    "from sklearn import preprocessing\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db135034-34ff-4556-a304-d4238cace791",
   "metadata": {},
   "source": [
    "# Loding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "52405aad-f1bf-4c5d-bdfb-257da3356aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('milknew.csv') #loading data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6693f28e-8220-4989-bad3-02a2ed627054",
   "metadata": {},
   "source": [
    "# Explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e37a6728-92da-4d9d-93e1-cfe0f8526f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pH</th>\n",
       "      <th>Temprature</th>\n",
       "      <th>Taste</th>\n",
       "      <th>Odor</th>\n",
       "      <th>Fat</th>\n",
       "      <th>Turbidity</th>\n",
       "      <th>Colour</th>\n",
       "      <th>Grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.6</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>254</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.6</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>253</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>246</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.5</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>255</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.6</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>255</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pH  Temprature  Taste  Odor  Fat   Turbidity  Colour   Grade\n",
       "0  6.6          35      1     0     1          0     254    high\n",
       "1  6.6          36      0     1     0          1     253    high\n",
       "2  8.5          70      1     1     1          1     246     low\n",
       "3  9.5          34      1     1     0          1     255     low\n",
       "4  6.6          37      0     0     0          0     255  medium"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head() # checking first 5 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a65e7e09-c266-416c-9531-94d26cc104a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1059, 8)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape # checking number of rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5c6812cb-acda-49c9-bb2e-8285f7639817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['pH', 'Temprature', 'Taste', 'Odor', 'Fat ', 'Turbidity', 'Colour',\n",
       "       'Grade'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns #chcecking column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e8eccbcd-f6a4-4eac-b245-0c312ef57e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pH</th>\n",
       "      <th>Temprature</th>\n",
       "      <th>Taste</th>\n",
       "      <th>Odor</th>\n",
       "      <th>Fat</th>\n",
       "      <th>Turbidity</th>\n",
       "      <th>Colour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1059.000000</td>\n",
       "      <td>1059.000000</td>\n",
       "      <td>1059.000000</td>\n",
       "      <td>1059.000000</td>\n",
       "      <td>1059.000000</td>\n",
       "      <td>1059.000000</td>\n",
       "      <td>1059.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.630123</td>\n",
       "      <td>44.226629</td>\n",
       "      <td>0.546742</td>\n",
       "      <td>0.432483</td>\n",
       "      <td>0.671388</td>\n",
       "      <td>0.491029</td>\n",
       "      <td>251.840415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.399679</td>\n",
       "      <td>10.098364</td>\n",
       "      <td>0.498046</td>\n",
       "      <td>0.495655</td>\n",
       "      <td>0.469930</td>\n",
       "      <td>0.500156</td>\n",
       "      <td>4.307424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>240.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.500000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>250.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.700000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>255.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.800000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>255.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.500000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>255.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                pH   Temprature        Taste         Odor         Fat   \\\n",
       "count  1059.000000  1059.000000  1059.000000  1059.000000  1059.000000   \n",
       "mean      6.630123    44.226629     0.546742     0.432483     0.671388   \n",
       "std       1.399679    10.098364     0.498046     0.495655     0.469930   \n",
       "min       3.000000    34.000000     0.000000     0.000000     0.000000   \n",
       "25%       6.500000    38.000000     0.000000     0.000000     0.000000   \n",
       "50%       6.700000    41.000000     1.000000     0.000000     1.000000   \n",
       "75%       6.800000    45.000000     1.000000     1.000000     1.000000   \n",
       "max       9.500000    90.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "         Turbidity       Colour  \n",
       "count  1059.000000  1059.000000  \n",
       "mean      0.491029   251.840415  \n",
       "std       0.500156     4.307424  \n",
       "min       0.000000   240.000000  \n",
       "25%       0.000000   250.000000  \n",
       "50%       0.000000   255.000000  \n",
       "75%       1.000000   255.000000  \n",
       "max       1.000000   255.000000  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe() #Statistical Summary of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9ebc7460-0b4b-497f-92d6-d3aec468a1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1059 entries, 0 to 1058\n",
      "Data columns (total 8 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   pH          1059 non-null   float64\n",
      " 1   Temprature  1059 non-null   int64  \n",
      " 2   Taste       1059 non-null   int64  \n",
      " 3   Odor        1059 non-null   int64  \n",
      " 4   Fat         1059 non-null   int64  \n",
      " 5   Turbidity   1059 non-null   int64  \n",
      " 6   Colour      1059 non-null   int64  \n",
      " 7   Grade       1059 non-null   object \n",
      "dtypes: float64(1), int64(6), object(1)\n",
      "memory usage: 66.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info() #information abour data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be1d760-7eea-4a05-8a5d-c1268ca42661",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "69881c42-d259-491f-8be9-1d16208e8497",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [s.strip().replace(' ', '_') for s in df.columns] #replacing colums names which has spaces with the \"_\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c087d6cd-9c84-4dad-ad2e-aa8810a25b7e",
   "metadata": {},
   "source": [
    "#### We have changed the coloumn names here in a more readable and reliable way by adding an underscore in between the spaces. This will be more lookable and ideal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f166920c-b28b-4b83-b375-1c65f3f0af1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pH            0\n",
       "Temprature    0\n",
       "Taste         0\n",
       "Odor          0\n",
       "Fat           0\n",
       "Turbidity     0\n",
       "Colour        0\n",
       "Grade         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum() #Checking for null values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5dd92d-ef79-43e2-bf13-d2592b567427",
   "metadata": {},
   "source": [
    "### We dont have any NA values in dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fce21d98-2a25-4641-b635-1fd2ddc72c37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pH            float64\n",
       "Temprature      int64\n",
       "Taste           int64\n",
       "Odor            int64\n",
       "Fat             int64\n",
       "Turbidity       int64\n",
       "Colour          int64\n",
       "Grade          object\n",
       "dtype: object"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes # checking data types of colunms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c051b77-cd86-4413-8362-2d95603c137b",
   "metadata": {},
   "source": [
    "# Transform data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4a1061-a5dd-41cb-89b7-d80fcb0d6eef",
   "metadata": {},
   "source": [
    "#### Change the datatype to category which is feasible to find the relationship between the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ec3a4011-2a7a-49e9-9175-a1e1d5803766",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Grade']=df['Grade'].astype('category') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "02395b98-f1b7-4582-9442-bcd6ecf08417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pH             float64\n",
       "Temprature       int64\n",
       "Taste            int64\n",
       "Odor             int64\n",
       "Fat              int64\n",
       "Turbidity        int64\n",
       "Colour           int64\n",
       "Grade         category\n",
       "dtype: object"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes # checking data types of colunms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff52c8f-cb19-4321-a776-903586873e15",
   "metadata": {},
   "source": [
    "#### We have performed ordinal encoding on the predictor variables here to change the category type variables to an integer value. This will help the model to improve the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d11d1768-09a9-4da4-b9b8-566b2710ecde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pH</th>\n",
       "      <th>Temprature</th>\n",
       "      <th>Taste</th>\n",
       "      <th>Odor</th>\n",
       "      <th>Fat</th>\n",
       "      <th>Turbidity</th>\n",
       "      <th>Colour</th>\n",
       "      <th>Grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.6</td>\n",
       "      <td>35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>254</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.6</td>\n",
       "      <td>36</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>253</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>246</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.5</td>\n",
       "      <td>34</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>255</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.6</td>\n",
       "      <td>37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>255</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pH  Temprature  Taste  Odor  Fat  Turbidity  Colour   Grade\n",
       "0  6.6          35    1.0   0.0  1.0        0.0     254    high\n",
       "1  6.6          36    0.0   1.0  0.0        1.0     253    high\n",
       "2  8.5          70    1.0   1.0  1.0        1.0     246     low\n",
       "3  9.5          34    1.0   1.0  0.0        1.0     255     low\n",
       "4  6.6          37    0.0   0.0  0.0        0.0     255  medium"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp=OrdinalEncoder()\n",
    "df.Taste=temp.fit_transform(df[['Taste']])\n",
    "df.Odor=temp.fit_transform(df[['Odor']])\n",
    "df.Fat=temp.fit_transform(df[['Fat']])\n",
    "df.Turbidity=temp.fit_transform(df[['Turbidity']])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "82916f51-4055-4f6f-801b-9790c732b717",
   "metadata": {},
   "outputs": [],
   "source": [
    "Category_Features = ['Taste','Odor', 'Fat','Turbidity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a22ea092-9317-4418-aca1-37b1967eb4ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pH            float64\n",
       "Temprature      int64\n",
       "Taste         float64\n",
       "Odor          float64\n",
       "Fat           float64\n",
       "Turbidity     float64\n",
       "Colour          int64\n",
       "Grade           int32\n",
       "dtype: object"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = LabelEncoder() \n",
    "df[target]=enc.fit_transform(df[target])\n",
    "df.dtypes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eace4a8-81aa-442e-af8a-8139d2d9464b",
   "metadata": {},
   "source": [
    "#### Checking the Number of groups in target column and there count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2c7f2d87-8ea2-455b-95d7-146f954e11c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    429\n",
       "2    374\n",
       "0    256\n",
       "Name: Grade, dtype: int64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Grade'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "10009207-7c08-4b76-a9d2-db86d8864ba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1059, 8)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fb7f1ffd-cafe-4e2b-96c6-70a1ab8cab9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training set and the test set \n",
    "target = 'Grade'\n",
    "predictors = list(df.columns) #Assigning Predictors and coverting pandas series object to list.\n",
    "predictors.remove(target)\n",
    "X=df[predictors]\n",
    "y=df[target]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326b063f-d10e-4228-97aa-0d0d20233982",
   "metadata": {},
   "source": [
    "# Split data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f971e0a1-33e4-4e80-9db9-7986f284a4ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    287\n",
       "2    269\n",
       "0    185\n",
       "Name: Grade, dtype: int64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "train_y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee68cb54-d9e1-489d-b87f-08db2c973fc5",
   "metadata": {},
   "source": [
    "# Data Balacing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f9053e-f171-4ad5-996d-5d806f9c96b3",
   "metadata": {},
   "source": [
    "#### Since, the data is not balanced with respect to the target variable, we have performed the oversampling on the data to avoid the discrepancy between observations of the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b3fc477f-ecf2-42a3-938e-0eeab324e626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    287\n",
       "0    287\n",
       "2    287\n",
       "Name: Grade, dtype: int64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ros = RandomOverSampler(random_state=1)\n",
    "train_X, train_y = ros.fit_resample(train_X, train_y)\n",
    "train_y.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "02d2ac08-90ed-4071-a41a-8ce9a96d86b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pH</th>\n",
       "      <th>Temprature</th>\n",
       "      <th>Colour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>40</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.6</td>\n",
       "      <td>45</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.5</td>\n",
       "      <td>70</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.5</td>\n",
       "      <td>45</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.6</td>\n",
       "      <td>37</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>6.8</td>\n",
       "      <td>45</td>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>857</th>\n",
       "      <td>6.8</td>\n",
       "      <td>45</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>858</th>\n",
       "      <td>6.5</td>\n",
       "      <td>36</td>\n",
       "      <td>247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>6.5</td>\n",
       "      <td>38</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>6.5</td>\n",
       "      <td>36</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>861 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pH  Temprature  Colour\n",
       "0    3.0          40     255\n",
       "1    6.6          45     250\n",
       "2    8.5          70     246\n",
       "3    5.5          45     250\n",
       "4    6.6          37     255\n",
       "..   ...         ...     ...\n",
       "856  6.8          45     240\n",
       "857  6.8          45     255\n",
       "858  6.5          36     247\n",
       "859  6.5          38     255\n",
       "860  6.5          36     255\n",
       "\n",
       "[861 rows x 3 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in Category_Features: #Removing \n",
    "    predictors.remove(i)\n",
    "predictors #Checking Predictors list.\n",
    "train_X[predictors]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07e726f-7065-463d-9b96-e7d2402a8c52",
   "metadata": {},
   "source": [
    "#  Standardzation of quantitative features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "08cb431f-6fca-4697-ae9a-89f64f04bc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create a standard scaler and fit it to the training set of predictors\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(train_X[predictors])\n",
    "\n",
    "# Transform the predictors of training, validation and newCustomer\n",
    "train_scaled_data= scaler.transform(train_X[predictors])\n",
    "test_scaled_data = scaler.transform(test_X[predictors])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78731985-bec0-4363-a8f8-b0db6938fc32",
   "metadata": {},
   "source": [
    "#### Here we are adding back the Standardized values back to dataframe after scaling them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "580ce42d-b099-44b8-a46d-b05abdaebe29",
   "metadata": {},
   "outputs": [],
   "source": [
    "s=0\n",
    "for i in predictors:\n",
    "    train_X[i]=train_scaled_data[:,s]\n",
    "    test_X[i]=test_scaled_data[:,s]\n",
    "    s=s+1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49c25d6-69f9-450e-a144-d2a1e1d6b9ff",
   "metadata": {},
   "source": [
    "#### Since, continuous variables are measured in different scales we performed standardization to get them into the same scale. Here we have performed the same on train_predictors and validation_predictors. Firstly, we have removed our categorical variables and the target variable, since we should not perform standardizing on them. Later, we will append thoe category features list to the standardized dataframe. Finally, we get a standardized dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ca1b08-a01a-45d4-97ef-da997e1afb1a",
   "metadata": {},
   "source": [
    "# Prediction with K-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2a3beab6-902f-4fa2-b82d-2b9cef424970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using default parameters : 0.9748427672955975\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "_ = knn.fit(train_X, train_y)\n",
    "y_pred = knn.predict(test_X)\n",
    "print('Accuracy using default parameters :',accuracy_score(test_y,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9eee3180-ea67-4cc9-af32-23f10525db83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.9988372093023254\n",
      "parameters:  {'metric': 'euclidean', 'n_neighbors': 11, 'weights': 'distance'}\n"
     ]
    }
   ],
   "source": [
    "score_measure = 'accuracy'\n",
    "k_fold = 10\n",
    "# Start with an initial guess for parameters\n",
    "param_grid = {\n",
    "     'n_neighbors': list(range(1,round(np.sqrt(len(df))),2)),\n",
    "    'metric': ['euclidean', 'cosine','manhattan','minkowski'],\n",
    "    'weights':['uniform','distance']\n",
    "}\n",
    "gridSearch = GridSearchCV(KNeighborsClassifier(), param_grid, cv=k_fold, scoring=score_measure,\n",
    "                          n_jobs=-1,error_score='raise')  # n_jobs=-1 will utilize all available CPUs \n",
    "grid_result=gridSearch.fit(train_X, train_y)\n",
    "Y_predict=gridSearch.predict(test_X)\n",
    "print(score_measure, 'score: ', gridSearch.best_score_)\n",
    "print('parameters: ', gridSearch.best_params_)\n",
    "\n",
    "Best_accuracy_knn = grid_result.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaecba2-22fb-4113-969a-bedf72b08700",
   "metadata": {},
   "source": [
    "\n",
    "#### K-NN is a supervised machine learning algorithm that can solve both classification and regression problems. This is one of the simplest yet most powerful algorithms. Save the training data instead of learning the discriminant function from the training data. It is also called a \"lazy algorithm\" for the same reason.\n",
    "#### The parameters that we have used here are :\n",
    "#### 1. n_neighbours: The range of N number will be odd number from 1 to square root of number of observations.\n",
    "#### 2. metric: The metric we used here is Euclidian distance.\n",
    "#### 3. Weights: Weighted kNN is a modified version of k nearest neighbors.\n",
    "#### Here, the training data means 70 % of the actual data, including the target variable. We are implementing the k-nn model on this data and finding the accuracy of the data set .After performing hyperparameter tuning when k-fold value is 10, we got the best model at n_estimators: ,metric, Weights: with Accuracy of Accuracy score: 0.9988372093023254."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53fea5d-6878-489e-bb89-bf0cb9d22449",
   "metadata": {},
   "source": [
    "# Split data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cc073f5d-96bb-4576-bea6-747ef14e5cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the training set and the test set \n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "edae67df-229e-49ff-bc15-db9356491282",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Here I am performing Data split again because for knn we did standaiztion of predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f30c52-c67c-4aec-9cb3-ddb6a158dfe1",
   "metadata": {},
   "source": [
    "# Prediction with Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5ec56792-e9ff-45fa-91fb-78da998f015e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using default parameters : 1.0\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree \n",
    "dtree=DecisionTreeClassifier(random_state=3)\n",
    "_ = dtree.fit(train_X, train_y)\n",
    "y_pred = dtree.predict(test_X)\n",
    "print('Accuracy using default parameters :',accuracy_score(test_y, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d536c499-b327-4dcf-a3c9-372568751e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 750 candidates, totalling 7500 fits\n",
      "Accuracy score:  0.9892072072072073\n",
      "parameters:  {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 2}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "param_grid = {\n",
    "\n",
    "    'criterion':  ['gini','entropy','log_loss'],\n",
    "    'max_depth':  [2]+list(range(5,50,5)),\n",
    "     'min_samples_leaf': range(2,len(train_X.columns),1) , \n",
    "     'min_samples_split': range(2,len(train_X.columns),1)\n",
    "    \n",
    "\n",
    "}\n",
    "\n",
    "best_grid_search_model_DT = GridSearchCV(estimator=DecisionTreeClassifier(),param_grid=param_grid,cv=10, n_jobs=-1,verbose=1)\n",
    "\n",
    "_ = best_grid_search_model_DT.fit(train_X, train_y)\n",
    "\n",
    "print('Accuracy', 'score: ', best_grid_search_model_DT.best_score_)\n",
    "\n",
    "print('parameters: ', best_grid_search_model_DT.best_params_)\n",
    "Best_accuracy_DT = best_grid_search_model_DT.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf94091c-0003-47d6-9ec9-91c2b2df4102",
   "metadata": {},
   "source": [
    "#### The default decision tree model tends to overfit the data and got an accuracy score of 1. To overcome the model's overfitting, we performed pruning. We have tuned a decision tree on the\n",
    "- #### maximum depth that we allow the tree to grow,min_Samples_split The bare minimum of samples needed to split an internal, \n",
    "- #### min_samples_leaf The minimum number of samples that we can specify to term a given node as a leaf node so that we do not want to split it further is\n",
    "- #### min_samples_split The minimum number of samples required to split, \n",
    "- #### The criterion of has different functions that measure the quality of a split, Gini,entropy,log_loss. \n",
    "#### After performing hyperparameter tuning when k-fold value is 10, we got the best model at 'criterion': 'gini' , 'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 2 with Accuracy of Accuracy score: 0.9892072072072073. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293eb937-574f-4f4c-9305-7c12694a69d1",
   "metadata": {},
   "source": [
    "# Prediction with Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "433adc67-96f1-4530-94b1-2aeb50c5e649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using default parameters : 1.0\n"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "rforest = RandomForestClassifier(random_state=1)\n",
    "_ = rforest.fit(train_X, train_y)\n",
    "y_pred = rforest.predict(test_X)\n",
    "print('Accuracy using default parameters :',accuracy_score(test_y, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7ab5ea3e-0d3a-419d-889f-880a6b157007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 432 candidates, totalling 4320 fits\n",
      "Accuracy score:  0.9972972972972972\n",
      "parameters:  {'max_depth': 7, 'max_features': 0.1, 'max_samples': 0.5, 'n_estimators': 20}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "param_grid = {\n",
    "    \"n_estimators\": range(20,100,20),\n",
    "    'max_features':np.arange(0.1, 1, 0.1),\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'max_samples': [0.3, 0.5, 0.8], \n",
    "\n",
    "         }\n",
    "            \n",
    "best_grid_search_model = GridSearchCV(estimator=RandomForestClassifier(random_state=1), \n",
    "                                    scoring='accuracy', param_grid=param_grid, cv=10, verbose=1,  n_jobs = -1,error_score='raise')\n",
    "_ = best_grid_search_model.fit(train_X, train_y)\n",
    "print('Accuracy', 'score: ', best_grid_search_model.best_score_)\n",
    "print('parameters: ', best_grid_search_model.best_params_)\n",
    "Best_accuracy_RF = best_grid_search_model.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5bd9a9-eaf0-41ba-adad-a7f4fabfc15c",
   "metadata": {},
   "source": [
    "#### Overfitting is prevented by using random forests, and produce results based on majority ranking.\n",
    "#### The Parameters that we have used here are :\n",
    "#### 1. n_estimators : Number of trees we want to build before taking the maximum averages of predictions. Though, higher number of trees improves the performance, it will make the code to run slower. We have taken an ideal range of 20 to 100.\n",
    "#### 2. max_features : The maximum number of features Random Forest is allowed to try in individual tree.\n",
    "#### 3. max_depth : Here, we have considered the longest path between the root node and leaf node in a list.\n",
    "#### 4. max_samples : We have taken the sample size in a list with an iteration ranges from 30 percent to 80 percent.\n",
    "#### By calling the Random forest function setting all of these parameters, we could find the optimal model at the 'max_depth': 7, 'max_features': 0.1, 'max_samples': 0.5, 'n_estimators': 20 with an accuracy of 0.9972972972972972."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1477e19f-e861-436b-8c4d-9f08ac6b5d9e",
   "metadata": {},
   "source": [
    "# Prediction with AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "316f648c-29a8-4a77-b956-e7bf7bc70fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using default parameters : 0.9276729559748428\n"
     ]
    }
   ],
   "source": [
    "#AdaBoostClassifier\n",
    "adaboost = AdaBoostClassifier(random_state=3)\n",
    "_ = adaboost.fit(train_X, train_y)\n",
    "y_pred = adaboost.predict(test_X)\n",
    "print('Accuracy using default parameters :',accuracy_score(test_y, y_pred))\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7efbb770-b47d-4388-8bca-5ddf319d2dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 196 candidates, totalling 1960 fits\n",
      "accuracy score:  0.9527207207207209\n",
      "parameters:  {'learning_rate': 1.0, 'n_estimators': 11}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "             'learning_rate': [0.7,0.8,0.9,1.0],\n",
    "           'n_estimators': range(1,50,1)\n",
    "              }\n",
    "best_grid_search_model = GridSearchCV(estimator=AdaBoostClassifier(random_state=3),\n",
    "                                    scoring='accuracy', param_grid=param_grid, cv=10, verbose=1,  n_jobs = -1)\n",
    "_ = best_grid_search_model.fit(train_X, train_y)\n",
    "print('accuracy', 'score: ', best_grid_search_model.best_score_)\n",
    "print('parameters: ', best_grid_search_model.best_params_)\n",
    "Best_accuracy_AdaBoost = best_grid_search_model.best_score_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f92d556-d686-43cf-a26a-e69858157940",
   "metadata": {},
   "source": [
    "#### The default adaboost model has an accuracy score of 0.9276729559748428 and tends to overfit the data. Pruning was used to overcome the model's overfitting. We customized adaboost using the three most critical parameters: n estimators, learning rate, and loss.\n",
    "- #### n_estimators : Number of trees we want to build before taking the maximum averages of predictions. Though, higher number of trees improves the performance, it will make the code to run slower. We have taken an ideal range of 1 to 50.\n",
    "- #### learning rate: learning rate represents each model's contribution to the weights and is set to 1 by default. When the learning rate is reduced, the weights are slightly increased or lowered, causing the model train to go slower (but sometimes resulting in better performance scores).\n",
    "- #### loss: leave it at default because the loss is unique to AdaBoostRegressor and determines the loss function to employ while updating weights. This is set to a linear loss function by default, but it may be altered to a square or exponential loss function.\n",
    "#### After performing hyperparameter adjustment with a k-fold value of 10, we achieved the best model with parameters 'learning rate': 1.0, 'n estimators': 11, leaving loss function to default, and Accuracy of Accuracy score: 0.9527207207207209."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d44428f-fe7b-45c2-b073-af30a745db04",
   "metadata": {},
   "source": [
    "# Prediction with GradientBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1c4f8ae8-d771-4a58-8c67-5d4ee56f8381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using default parameters : 1.0\n"
     ]
    }
   ],
   "source": [
    "#GradientBoosting\n",
    "gboost = GradientBoostingClassifier(random_state=3)\n",
    "_ = gboost.fit(train_X, train_y)\n",
    "y_pred = gboost.predict(test_X)\n",
    "print('Accuracy using default parameters :',accuracy_score(test_y, y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8d1c47c3-f345-40fa-b960-d154f5da1164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 80 candidates, totalling 800 fits\n",
      "accuracy score:  0.9972972972972972\n",
      "parameters:  {'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "param_grid = { \n",
    "               'n_estimators' : [25, 50 ,75, 100, 200],\n",
    "              'learning_rate': [0.005 ,0.05, 0.5, 1.5],\n",
    "              'max_depth': [2, 4, 6, 8],\n",
    "       \n",
    "              }\n",
    "best_grid_search_model = GridSearchCV(estimator=GradientBoostingClassifier(random_state=3), \n",
    "                                    scoring='accuracy', param_grid=param_grid, cv=10, verbose=1,  n_jobs = -1)\n",
    "_ = best_grid_search_model.fit(train_X, train_y)\n",
    "print('accuracy', 'score: ', best_grid_search_model.best_score_)\n",
    "print('parameters: ', best_grid_search_model.best_params_)\n",
    "Best_accuracy_GB = best_grid_search_model.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e6eded-a0b2-4b8c-99c4-9f336d3bd60c",
   "metadata": {},
   "source": [
    "#### Gradient boosting is a powerful technique for developing predictive models. Gradient boosting is a greedy algorithm that can quickly overfit a training dataset. It can benefit from regularization methods that penalize different parts of the algorithm and improve overall algorithm performance by reducing overfitting.\n",
    "#### Tunign n_estimators and Learning rate:\n",
    "- #### The number of trees we add to the model is recorded by the variable n estimators. It can be costly computationally to process many trees. In general, n estimators should be changed to reflect changes in learning rate (a 10-fold drop in learning rate should correspond to an approximately 10-fold increase in n_estimators).\n",
    "- #### It can be said from the low learning_rate value of 0.05 that the n_estimators value is going to be high that seems to be the case as the best n_estimators ended up being : 100.\n",
    "- #### max_depth. This indicates how deep the built tree can be. The deeper the tree, the more splits it has and it captures more information about how the data. \n",
    "#### The default Gradiant Bossting model tends to overfit the data and got an accuracy score of 1. To overcome the model's overfitting, we performed hypertuning. We have tuned gradiant boosting on the maximum depth that we allow the tree to grow, After performing hyperparameter tuning we got the best model at learning_rate': 0.05 , 'max_depth': 4, n_estimators': 100, with Accuracy of Accuracy score: 0.9972972972972972."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482dc0b3-252b-4798-8bf1-6767e8217a44",
   "metadata": {},
   "source": [
    "# Prediction with XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "020b3580-225d-4f63-bc69-f4bad5964113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using default parameters : 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#XGBClassifier\n",
    "xgboost = XGBClassifier(random_state=3)\n",
    "_ = xgboost.fit(train_X, train_y)\n",
    "y_pred = xgboost.predict(test_X)\n",
    "print('Accuracy using default parameters :',accuracy_score(test_y, y_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "65f87b7f-87cc-43fa-b05f-1e49091f0b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.9986486486486486\n",
      "parameters:  {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 120}\n"
     ]
    }
   ],
   "source": [
    "param_grid = { \n",
    "              'max_depth': range(2,10,1),\n",
    "           'learning_rate': [0.2,0.1, 0.01, 0.05],\n",
    "           'n_estimators': range(40,220,40)\n",
    "              }\n",
    "best_grid_search_model = GridSearchCV(estimator=XGBClassifier(random_state=3), \n",
    "                                    scoring='accuracy', param_grid=param_grid, cv=10, verbose=0,  n_jobs = -1)\n",
    "_ = best_grid_search_model.fit(train_X, train_y)\n",
    "print('accuracy', 'score: ', best_grid_search_model.best_score_)\n",
    "print('parameters: ', best_grid_search_model.best_params_)\n",
    "Best_accuracy_XGB = best_grid_search_model.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2439c0c8-eb14-43f6-a0ba-641863ef5680",
   "metadata": {},
   "source": [
    "#### Extreme Gradient Boosting (XGBoost) is a distributed, scalable gradient-boosted decision tree (GBDT) machine learning framework. The top machine learning library for regression, classification, and ranking issues, it offers parallel tree boosting.\n",
    "#### Here we used max depth, learning_rate, and n_estimators as parameters. Max depth is the maximum depth of the tree. learning_rate parameter can be set to control the weighting of new trees added to the model. n estimators are the number of runs the model will try to learn. After hyper tuning, the best values for parameters are max depth: 4, learning rate: 0.1, and n estimators:120. Hence, we obtained the accuracy = 0.9986486486486486"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ceeb14-2d6d-483a-942b-be8e990e5a84",
   "metadata": {},
   "source": [
    "# Summarize results    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1fa23b01-c317-4d27-81cb-e8a9ce6c2b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy scores...\n",
      "K-NN:             0.9988372093023254\n",
      "Decision Tree:    0.9892072072072073\n",
      "Random Forest:    0.9972972972972972\n",
      "Ada Boosted:      0.9527207207207209\n",
      "GradientBoosting: 0.9972972972972972\n",
      "XGBoost:          0.9986486486486486\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy scores...\")\n",
    "print(f\"{'K-NN:':11}       {Best_accuracy_knn}\")\n",
    "print(f\"{'Decision Tree:':11}    {Best_accuracy_DT}\")\n",
    "print(f\"{'Random Forest:':11}    {Best_accuracy_RF}\")\n",
    "print(f\"{'Ada Boosted:':11}      {Best_accuracy_AdaBoost}\")\n",
    "print(f\"{'GradientBoosting:':11} {Best_accuracy_GB}\")\n",
    "print(f\"{'XGBoost:':11}       {Best_accuracy_XGB}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48935775-63db-438b-a587-bfec37ddc21e",
   "metadata": {},
   "source": [
    "# Business conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259d1470-9a93-4c57-887a-5a759267ad88",
   "metadata": {},
   "source": [
    "- #### Overall the quality of the data is excellent. This is evident from the fact that we were able to accurately predict the output using various models every time.      \n",
    "- #### We have used knn, decision tree, ada boost, random forest, gradient boost as well as xg boost to create prediction models.The least accuracy.we were able to output was 95.2% for ada boost.This level of accuracy is necessary while investigating food-related predictions, especially those which could cause a medical or legal problem.                    \n",
    "- #### Due to the nature of the split between low, medium, and high, and the fact that both FPs and FNs lead to potential loss(either through wastage or medical expenses due to food poisoning) led us to take accuracy as the metric that will be used to determine if a model is good or not.               \n",
    "- #### While all models generated good results K-NN classifier came out as the Best with an accuracy score of 99.88%.\n",
    "- #### K-NN is Recommended model to deployed for getting prediction on milk quality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
